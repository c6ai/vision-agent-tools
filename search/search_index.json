{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vision Agent Tools Documentation","text":"<p>This repository contains tools that solve vision problems. This tools can be used in conjunction with the vision-agent.</p>"},{"location":"clip_media_sim/","title":"CLIPMediaSim","text":""},{"location":"clip_media_sim/#video-similarity","title":"Video similarity","text":"<pre><code>from vision_agent_tools.models.clip_media_sim import CLIPMediaSim\nfrom decord import VideoReader\nfrom decord import cpu\nfrom PIL import Image\n\n# Path to your target image\nimage_path = \"path/to/your/image.jpg\"\n\n# Path to your video\nvideo_path = \"path/to/your/video.mp4\"\n\n# Load the image\ntarget_image = Image.open(image_path)\n\n# Load the video\nvr = VideoReader(video_path, ctx=cpu(0))\n\n# Subsample frames\nframe_idxs = range(0, len(vr) - 1, 20)\nframes = vr.get_batch(frame_idxs).asnumpy()\n\n# Calculate video timestamps\nvideo_time = len(vr) / vr.get_avg_fps()\n\n# Create the CLIPMediaSim instance\nclip_media_sim = CLIPMediaSim()\n\n# Run video similarity against the target image\nresults = clip_media_sim(video=frames, target_image=target_image)\n\n# The results should be a list of [index_of_frame, confidence_score] where the\n# video is similar to the target image.\n\n# To find the time at which a given frame happens, you can do the following\n\ntime_per_frame = video_time / len(frames)\n\ntimestamp = results[0][0] * time_per_frame\n\nprint(\"Similarity detection complete!\")\n</code></pre> <p>You can also run similarity against a target text doing the following:</p> <pre><code>results = clip_media_sim(video=frames, target_text=\"a turtle holding the earth\")\n</code></pre>"},{"location":"clip_media_sim/#vision_agent_tools.models.clip_media_sim.CLIPMediaSim","title":"<code>CLIPMediaSim</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>A class that receives a video and a target image or text and returns the frames that are most similar to the target.</p>"},{"location":"clip_media_sim/#vision_agent_tools.models.clip_media_sim.CLIPMediaSim.__call__","title":"<code>__call__(video, target_image=None, target_text=None, thresh=0.3)</code>","text":"<p>Receives a video and a target image or text and returns the frames that are most similar to the target.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>ndarray</code> <p>The input video to be processed.</p> required <code>target_image</code> <code>Image | None</code> <p>The target image to compare the video frames with.</p> <code>None</code> <code>target_text</code> <code>str | None</code> <p>The target text to compare the video frames with.</p> <code>None</code> <code>thresh</code> <code>float</code> <p>The threshold to filter the results. Defaults to 0.3.</p> <code>0.3</code>"},{"location":"clip_media_sim/#vision_agent_tools.models.clip_media_sim.CLIPMediaSim.__init__","title":"<code>__init__(device='cuda')</code>","text":"<p>Initializes the CLIPMediaSim object with a pre-trained CLIP model.</p>"},{"location":"controlnet_aux/","title":"Controlnet-Aux","text":""},{"location":"controlnet_aux/#pose-detector","title":"Pose Detector","text":"<pre><code>from PIL import Image\nfrom vision_agent_tools.models.controlnet_aux import Image2Pose\n\n# Path to your test image\ntest_image_path = \"path/to/your/image.jpg\"\n\n# Load the image\nimage = Image.open(test_image_path)\n# Create the Image2Pose instance\nimage_2_pose = Image2Pose()\n\n# Run pose detection and get the results\nresults = image_2_pose(image)\n\n# Optional: Save the result image (assuming results is a PIL Image)\n# results.save(\"result.png\")\n\nprint(\"Pose detection complete!\")\n</code></pre> Pose Detection Result"},{"location":"controlnet_aux/#vision_agent_tools.models.controlnet_aux.Image2Pose","title":"<code>Image2Pose</code>","text":"<p>A class that simplifies human pose detection using a pre-trained Openpose model.</p> <p>This class provides a convenient way to run pose detection on images using a pre-trained Openpose model from the <code>controlnet_aux</code> library. It takes a PIL Image object as input and returns the predicted pose information.</p>"},{"location":"controlnet_aux/#vision_agent_tools.models.controlnet_aux.Image2Pose.__call__","title":"<code>__call__(image)</code>","text":"<p>Performs pose detection on a PIL image and returns the results.</p> <p>This method takes a PIL Image object as input and runs the loaded Openpose detector on it. The predicted pose information is then resized to match the original image size and returned.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image for pose detection.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>PIL.Image: The image with the predicted pose information (format might vary       depending on the specific OpenposeDetector implementation).</p>"},{"location":"controlnet_aux/#vision_agent_tools.models.controlnet_aux.Image2Pose.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Image2Pose object with a pre-trained Openpose detector.</p> <p>This method loads a pre-trained Openpose model from the specified model hub (\"lllyasviel/Annotators\" in this case). The loaded detector is stored as an attribute for future use.</p>"},{"location":"depth_anything_v2/","title":"Depth-Anything-V2","text":"<p>This example demonstrates using the Depth-Anything-V2 tool for depth estimation on images.</p> <pre><code>from vision_agent_tools.models.depth_anything_v2 import DepthAnythingV2\n\n# (replace this path with your own!)\ntest_image = \"path/to/your/image.jpg\"\n\n# Load the image\nimage = Image.open(test_image)\n# Initialize the depth map estimation model.\ndepth_estimate = DepthAnythingV2()\n\n# Run the inference\nresults = depth_estimate(image)\n\n# Let's print the obtained depth map\nprint(results.map)\n</code></pre>"},{"location":"depth_anything_v2/#vision_agent_tools.models.depth_anything_v2.DepthAnythingV2","title":"<code>DepthAnythingV2</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Model for depth estimation using the Depth-Anything-V2 model from the paper Depth Anything V2.</p>"},{"location":"depth_anything_v2/#vision_agent_tools.models.depth_anything_v2.DepthAnythingV2.__call__","title":"<code>__call__(image)</code>","text":"<p>Depth-Anything-V2 is a highly practical solution for robust monocular depth estimation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image for object detection.</p> required <p>Returns:</p> Name Type Description <code>DepthMap</code> <code>DepthMap</code> <p>An object type containing a numpy array with the HxW depth map of the image.</p>"},{"location":"depth_anything_v2/#vision_agent_tools.models.depth_anything_v2.DepthAnythingV2.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Depth-Anything-V2 model.</p>"},{"location":"depth_anything_v2/#vision_agent_tools.models.depth_anything_v2.DepthMap","title":"<code>DepthMap</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the depth map of an image.</p> <p>Attributes:</p> Name Type Description <code>map</code> <code>Any</code> <p>HxW raw depth map of the image.</p>"},{"location":"florence2-sam2/","title":"Florence2Sam2","text":"<p>This tool uses FlorenceV2 and the SAM-2 model to do text to instance segmentation on image or video inputs.</p> <pre><code>from vision_agent_tools.models.florence2_sam2 import Florence2SAM2\nfrom decord import VideoReader\nfrom decord import cpu\n\n\n# Path to your video\nvideo_path = \"path/to/your/video.mp4\"\n\n# Load the video\nvr = VideoReader(video_path, ctx=cpu(0))\n\n# Subsample frames\nframe_idxs = range(0, len(vr) - 1, 20)\nframes = vr.get_batch(frame_idxs).asnumpy()\n\n# Create the Florence2SAM2 instance\nflorence2_sam2 = Florence2SAM2()\n\n# segment all the instances of the prompt \"ball\" for all video frames\nresults = florence2_sam2(video=frames, prompts=[\"ball\"])\n\n# Returns a dictionary where the first key is the frame index then an annotation\n# ID, then an object with the mask, label and possibly bbox (for images) for each\n# annotation ID. For example:\n# {\n#     0:\n#         {\n#             0: ImageBboxMaskLabel({\"mask\": np.ndarray, \"label\": \"car\"}),\n#             1: ImageBboxMaskLabel({\"mask\", np.ndarray, \"label\": \"person\"})\n#         },\n#     1: ...\n# }\n\nprint(\"Instance segmentation complete!\")\n</code></pre> <p>You can also run similarity against an image and get additionally bounding boxes doing the following:</p> <pre><code>results = florence2_sam2(image=image, prompts=[\"ball\"])\n</code></pre>"},{"location":"florence2-sam2/#vision_agent_tools.models.florence2_sam2.Florence2SAM2","title":"<code>Florence2SAM2</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>A class that receives a video or an image plus a list of text prompts and returns the instance segmentation for the text prompts in each frame.</p>"},{"location":"florence2-sam2/#vision_agent_tools.models.florence2_sam2.Florence2SAM2.__call__","title":"<code>__call__(prompts, image=None, video=None)</code>","text":"<p>Returns a dictionary where the first key is the frame index then an annotation ID, then an object with the mask, label and possibly bbox (for images) for each annotation ID. For example: {     0:         {             0: ImageBboxMaskLabel({\"mask\": np.ndarray, \"label\": \"car\"}),             1: ImageBboxMaskLabel({\"mask\", np.ndarray, \"label\": \"person\"})         },     1: ... }</p>"},{"location":"florence2-sam2/#vision_agent_tools.models.florence2_sam2.Florence2SAM2.__init__","title":"<code>__init__(device=None)</code>","text":"<p>Initializes the Florence2SAM2 object with a pre-trained Florencev2 model and a SAM2 model.</p>"},{"location":"florencev2-qa/","title":"FlorenceQA","text":"<p>This example demonstrates using the Florence2-QA tool to   to answer questions about images.</p> <p>NOTE: The FlorenceQA model can only be used in GPU environments.</p> <pre><code>from vision_agent_tools.models.florencev2_qa import FlorenceQA\n\n# (replace this path with your own!)\ntest_image = \"path/to/your/image.jpg\"\n\n# Load the image and create initialize the FlorenceQA model\nimage = Image.open(test_image)\nrun_florence_qa = FlorenceQA()\n\n# Time to put FlorenceQA to work! Let's pose a question about the image\nanswer = run_florence_qa(image, question=\"Is there a dog in the image?\")\n\n# Print the output answer\nprint(answer)\n</code></pre>"},{"location":"florencev2-qa/#vision_agent_tools.models.florencev2_qa.FlorenceQA","title":"<code>FlorenceQA</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>FlorenceQA is a tool that combines the Florence-2 and Roberta QA models to answer questions about images.</p> <p>NOTE: The Florence-2 model can only be used in GPU environments.</p>"},{"location":"florencev2-qa/#vision_agent_tools.models.florencev2_qa.FlorenceQA.__call__","title":"<code>__call__(image, question)</code>","text":"<p>FlorenceQA model answers questions about images.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The image to be analyzed.</p> required <code>question</code> <code>str</code> <p>The question to be answered.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The answer to the question.</p>"},{"location":"florencev2-qa/#vision_agent_tools.models.florencev2_qa.FlorenceQA.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the FlorenceQA model.</p>"},{"location":"florencev2/","title":"Florence-2","text":"<p>This example demonstrates using the Florence2 tool to interpret simple text prompts to perform tasks like captioning, object detection, and segmentation.</p> <p>NOTE: The Florence-2 model can only be used in GPU environments.</p> <pre><code>from vision_agent_tools.models.florencev2 import Florencev2, PromptTask\n\n# (replace this path with your own!)\ntest_image = \"path/to/your/image.jpg\"\n\n# Choose the task that you are planning to use\ntask_prompt = PromptTask.CAPTION\n\n# Load the image and create initialize the Florencev2 model\nimage = Image.open(test_image)\nrun_florence = Florencev2()\n\n# Time to put Florencev2 to work! Let's see what it finds...\nresults = run_florence(image=image, task=task_prompt)\n\n# Print the output result\nprint(f\"The image contains: {results[task_prompt]}\")\n</code></pre>"},{"location":"florencev2/#vision_agent_tools.models.florencev2.FlorenceV2ODRes","title":"<code>FlorenceV2ODRes</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for the  task."},{"location":"florencev2/#vision_agent_tools.models.florencev2.Florencev2","title":"<code>Florencev2</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation.</p> <p>NOTE: The Florence-2 model can only be used in GPU environments.</p>"},{"location":"florencev2/#vision_agent_tools.models.florencev2.Florencev2.__call__","title":"<code>__call__(task, image=None, video=None, prompt='')</code>","text":"<p>Florence-2 model sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model. For more examples and details, refer to the Florence-2 sample usage.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image for object detection.</p> <code>None</code> <code>task</code> <code>PromptTask</code> <p>The task to be performed on the image.</p> required <code>prompt</code> <code>Optional[str]</code> <p>The text input that complements the prompt task.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The output of the Florence-2 model based on the task and prompt.</p>"},{"location":"florencev2/#vision_agent_tools.models.florencev2.Florencev2.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Florence-2 model.</p>"},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask","title":"<code>PromptTask</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid task_prompts options for the Florence2 model.</p>"},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.CAPTION","title":"<code>CAPTION = '&lt;CAPTION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.CAPTION_TO_PHRASE_GROUNDING","title":"<code>CAPTION_TO_PHRASE_GROUNDING = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.DENSE_REGION_CAPTION","title":"<code>DENSE_REGION_CAPTION = '&lt;DENSE_REGION_CAPTION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.DETAILED_CAPTION","title":"<code>DETAILED_CAPTION = '&lt;DETAILED_CAPTION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.MORE_DETAILED_CAPTION","title":"<code>MORE_DETAILED_CAPTION = '&lt;MORE_DETAILED_CAPTION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.OBJECT_DETECTION","title":"<code>OBJECT_DETECTION = '&lt;OD&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.OCR","title":"<code>OCR = '&lt;OCR&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.OCR_WITH_REGION","title":"<code>OCR_WITH_REGION = '&lt;OCR_WITH_REGION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.OPEN_VOCABULARY_DETECTION","title":"<code>OPEN_VOCABULARY_DETECTION = '&lt;OPEN_VOCABULARY_DETECTION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.REFERRING_EXPRESSION_SEGMENTATION","title":"<code>REFERRING_EXPRESSION_SEGMENTATION = '&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.REGION_PROPOSAL","title":"<code>REGION_PROPOSAL = '&lt;REGION_PROPOSAL&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.REGION_TO_CATEGORY","title":"<code>REGION_TO_CATEGORY = '&lt;REGION_TO_CATEGORY&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.REGION_TO_DESCRIPTION","title":"<code>REGION_TO_DESCRIPTION = '&lt;REGION_TO_DESCRIPTION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"florencev2/#vision_agent_tools.models.florencev2.PromptTask.REGION_TO_SEGMENTATION","title":"<code>REGION_TO_SEGMENTATION = '&lt;REGION_TO_SEGMENTATION&gt;'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"internlm_xcomposer2/","title":"InternLM-XComposer-2.5","text":"<p>This example demonstrates how to use the InternLM-XComposer-2.5 tool to   to answer questions about images or videos.</p> <p>NOTE: The InternLM-XComposer-2.5 model should be used in GPU environments.</p> <pre><code>from decord import VideoReader\nfrom decord import cpu\nfrom vision_agent_tools.models.internlm_xcomposer2 import InternLMXComposer2\n\n# (replace this path with your own!)\nvideo_path = \"path/to/your/my_video.mp4\"\n\n# Load the video\nvr = VideoReader(video_path, ctx=cpu(0))\n# - subsample frames\nframe_idxs = range(0, len(vr) - 1, 2)\np_video = vr.get_batch(frame_idxs).asnumpy()\n\n# Initialize the InternLMXComposer2 model\nrun_inference = InternLMXComposer2()\nprompt = \"Here are some frames of a video. Describe this video in detail\"\n# Time to put InternLMXComposer2 to work!\nanswer = run_inference(video=p_video, prompt=prompt)\n\n# Print the output answer\nprint(answer)\n</code></pre>"},{"location":"internlm_xcomposer2/#vision_agent_tools.models.internlm_xcomposer2.InternLMXComposer2","title":"<code>InternLMXComposer2</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>InternLM-XComposer-2.5 is a tool that excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities.</p> <p>NOTE: The InternLM-XComposer-2.5 model should be used in GPU environments.</p>"},{"location":"internlm_xcomposer2/#vision_agent_tools.models.internlm_xcomposer2.InternLMXComposer2.__call__","title":"<code>__call__(prompt, image=None, video=None, frames=MAX_NUMBER_OF_FRAMES, chunk_length=None)</code>","text":"<p>InternLMXComposer2 model answers questions about a video or image.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt with the question to be answered.</p> required <code>image</code> <code>Image | None</code> <p>The image to be analyzed.</p> <code>None</code> <code>video</code> <code>VideoNumpy | None</code> <p>A numpy array containing the different images, representing the video.</p> <code>None</code> <code>frames</code> <code>int</code> <p>The number of frames to be used from the video.</p> <code>MAX_NUMBER_OF_FRAMES</code> <code>chunk_length</code> <code>int</code> <p>The number of frames for each chunk of video to analyze. The last chunk may have fewer frames.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The answers to the prompt.</p>"},{"location":"internlm_xcomposer2/#vision_agent_tools.models.internlm_xcomposer2.InternLMXComposer2.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the InternLMXComposer2.5 model.</p>"},{"location":"nsfw_classification/","title":"NSFW (Not Safe for Work) classification","text":"<p>This example demonstrates using the Not Safe for Work classification tool.</p> <pre><code>from vision_agent_tools.models.nsfw_classification import NSFWClassification\n\n# (replace this path with your own!)\ntest_image = \"path/to/your/image.jpg\"\n\n# Load the image\nimage = Image.open(test_image)\n# Initialize the NSFW model.\nnsfw_classification = NSFWClassification()\n\n# Run the inference\nresults = nsfw_classification(image)\n\n# Let's print the predicted label\nprint(results.label)\n</code></pre>"},{"location":"nsfw_classification/#vision_agent_tools.models.nsfw_classification.NSFWClassification","title":"<code>NSFWClassification</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images.</p>"},{"location":"nsfw_classification/#vision_agent_tools.models.nsfw_classification.NSFWClassification.__call__","title":"<code>__call__(image)</code>","text":"<p>Performs the NSFW inference on an image using the NSFWClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image for object detection.</p> required <p>Returns:</p> Name Type Description <code>NSFWInferenceData</code> <code>NSFWInferenceData</code> <p>The inference result from the NSFWClassification model. label (str): The label for the unsafe content detected in the image. score (float):The score for the unsafe content detected in the image.</p>"},{"location":"nsfw_classification/#vision_agent_tools.models.nsfw_classification.NSFWClassification.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the NSFW (Not Safe for Work) classification tool.</p>"},{"location":"nsfw_classification/#vision_agent_tools.models.nsfw_classification.NSFWInferenceData","title":"<code>NSFWInferenceData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an inference result from the NSFWClassification model.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>str</code> <p>The predicted label for the image.</p> <code>score</code> <code>float</code> <p>The confidence score associated with the prediction (between 0 and 1).</p>"},{"location":"nshot_counting/","title":"LOCA (Low-shot Object Counting network with iterative prototype Adaptation).","text":"<p>This example demonstrates how to use the NShot LOCA tool for object counting in images.</p> <pre><code>from vision_agent_tools.models.nshot_counting import NShotCounting\n\n# (replace this path with your own!)\ntest_image = \"path/to/your/image.jpg\"\n\n# Load the image\nimage = Image.open(test_image)\n# Initialize the counting model and choose the image output size you expect.\nObjectCounting = NShotCounting(zero_shot=False, img_size=512)\n\n# Run the inference\nresults = ObjectCounting(image, bbox=[12, 34, 56, 78])\n\n# Let's find out how many objects were found in total\nprint(\"Found a total count of {results.count} objects on the image!\")\n</code></pre>"},{"location":"nshot_counting/#vision_agent_tools.models.nshot_counting.CountingDetection","title":"<code>CountingDetection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an inference result from the LOCA model.</p> <p>Attributes:</p> Name Type Description <code>count</code> <code>int</code> <p>The predicted number of detected objects.</p> <code>masks</code> <code>list[Any]</code> <p>A list of numpy arrays representing the masks             of the detected objects in the image.</p>"},{"location":"nshot_counting/#vision_agent_tools.models.nshot_counting.NShotCounting","title":"<code>NShotCounting</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Model for object counting using the zeroshot and n-shot versions of the LOCA model from the paper A Low-Shot Object Counting Network With Iterative Prototype Adaptation .</p>"},{"location":"nshot_counting/#vision_agent_tools.models.nshot_counting.NShotCounting.__call__","title":"<code>__call__(image, bbox=None)</code>","text":"<p>LOCA injects shape and appearance information into object queries to precisely count objects of various sizes in densely and sparsely populated scenarios. It also extends to a zeroshot scenario and achieves excellent localization and count errors across the entire low-shot spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image for object detection.</p> required <code>bbox</code> <code>list[int]</code> <p>A list of four ints representing the bounding box coordinates (xmin, ymin, xmax, ymax)         of the detected query in the image.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CountingDetection</code> <code>CountingDetection</code> <p>An object type containing: - The count of the objects found similar to the bbox query. - A list of numpy arrays representing the masks of the objects found.</p>"},{"location":"nshot_counting/#vision_agent_tools.models.nshot_counting.NShotCounting.__init__","title":"<code>__init__(zero_shot=True, img_size=512)</code>","text":"<p>Initializes the LOCA model.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>int</code> <p>Size of the input image.</p> <code>512</code>"},{"location":"owlv2/","title":"OWLv2 Open-World Localization","text":"<p>This example demonstrates using the Owlv2 tool for object detection in images based on text prompts.</p> <pre><code>from vision_agent_tools.models.owlv2 import Owlv2\n\n# (replace this path with your own!)\ntest_image = \"path/to/your/image.jpg\"\n\n# What are you looking for? Write your detective prompts here!\nprompts = [\"a photo of a cat\", \"a photo of a dog\"]\n\n# Load the image and create your Owlv2 detective tool\nimage = Image.open(test_image)\nowlv2 = Owlv2()\n\n# Time to put Owlv2 to work! Let's see what it finds...\nresults = owlv2(image, prompts=prompts)\n\n# Did Owlv2 sniff out any objects? Let's see the results!\nif results:\n    for detection in results:\n        print(f\"Found it! It looks like a {detection.label} with a confidence of {detection.score:.2f}.\")\n        print(f\"Here's where it's hiding: {detection.bbox}\")\nelse:\n    print(\"Hmm, Owlv2 couldn't find anything this time. Maybe try a different prompt?\")\n</code></pre>"},{"location":"owlv2/#vision_agent_tools.models.owlv2.Owlv2","title":"<code>Owlv2</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Tool for object detection using the pre-trained Owlv2 model from Transformers.</p> <p>This tool takes an image and a list of prompts as input, performs object detection using the Owlv2 model, and returns a list of <code>Owlv2InferenceData</code> objects containing the predicted labels, confidence scores, and bounding boxes for detected objects with confidence exceeding a threshold.</p>"},{"location":"owlv2/#vision_agent_tools.models.owlv2.Owlv2.__call__","title":"<code>__call__(prompts, image=None, video=None)</code>","text":"<p>Performs object detection on an image using the Owlv2 model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image for object detection.</p> <code>None</code> <code>prompts</code> <code>list[str]</code> <p>A list of prompts to be used during inference.                   Currently, only one prompt is supported (list length of 1).</p> required <code>video</code> <code>Optional[VideoNumpy]</code> <p>The input video for object detection.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[list[Owlv2InferenceData]]</code> <p>Optional[list[Owlv2InferenceData]]: A list of <code>Owlv2InferenceData</code> objects containing the predicted                                labels, confidence scores, and bounding boxes for detected objects                                with confidence exceeding the threshold. Returns None if no objects                                are detected above the confidence threshold.</p>"},{"location":"owlv2/#vision_agent_tools.models.owlv2.Owlv2.__init__","title":"<code>__init__(model_config=None)</code>","text":"<p>Initializes the Owlv2 object detection tool.</p> <p>Loads the pre-trained Owlv2 processor and model from Transformers.</p>"},{"location":"owlv2/#vision_agent_tools.models.owlv2.Owlv2InferenceData","title":"<code>Owlv2InferenceData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an inference result from the Owlv2 model.</p>"},{"location":"owlv2/#vision_agent_tools.models.owlv2.Owlv2ProcessorWithNMS","title":"<code>Owlv2ProcessorWithNMS</code>","text":"<p>               Bases: <code>Owlv2Processor</code></p>"},{"location":"owlv2/#vision_agent_tools.models.owlv2.Owlv2ProcessorWithNMS.post_process_object_detection_with_nms","title":"<code>post_process_object_detection_with_nms(outputs, threshold=0.1, nms_threshold=0.3, target_sizes=None)</code>","text":"<p>Converts the raw output of [<code>OwlViTForObjectDetection</code>] into final bounding boxes in (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>[`OwlViTObjectDetectionOutput`]</code> <p>Raw outputs of the model.</p> required <code>threshold</code> <code>`float`, *optional*</code> <p>Score threshold to keep object detection predictions.</p> <code>0.1</code> <code>nms_threshold</code> <code>`float`, *optional*</code> <p>IoU threshold to filter overlapping objects the raw detections.</p> <code>0.3</code> <code>target_sizes</code> <code>`torch.Tensor` or `List[Tuple[int, int]]`, *optional*</code> <p>Tensor of shape <code>(batch_size, 2)</code> or list of tuples (<code>Tuple[int, int]</code>) containing the target size <code>(height, width)</code> of each image in the batch. If unset, predictions will not be resized.</p> <code>None</code> <p>Returns:     <code>List[Dict]</code>: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image     in the batch as predicted by the model.</p>"},{"location":"qr_reader/","title":"QR Reader","text":"<p>Tool for detecting QR codes in images.</p> <pre><code>from PIL import Image, ImageDraw\n\nfrom vision_agent_tools.models.qr_reader import QRReader\n\n# Open the image containing the QR code\nimage = Image.open(\"sample_qr_image.jpeg\")\n\n# Create a QR code reader object\nqr_reader = QRReader()\n\n# Detect QR codes in the image\ndetections = qr_reader(image)\n\n\nif detections:\n\n    detection = detections[0]\n    draw = ImageDraw.Draw(image)\n\n    # Print the detected text\n    print(f\"Decoded Text: {detection.text}\")\n\n    # Draw the bounding box\n    x_min, y_min, x_max, y_max = (\n        int(detection.bounding_box.x_min),\n        int(detection.bounding_box.y_min),\n        int(detection.bounding_box.x_max),\n        int(detection.bounding_box.y_max),\n    )\n    draw.rectangle(((x_min, y_min), (x_max, y_max)), outline=\"red\", width=2)\n\n    # Draw the text on top of the image\n    draw.text((x_min + 10, y_min - 10), detection.text, fill=\"blue\", anchor=\"mm\")\n    image.show()\nelse:\n    print(\"No QR codes detected in the image.\")\n</code></pre> Displaying the Detection Result"},{"location":"qr_reader/#vision_agent_tools.models.qr_reader.QRCodeDetection","title":"<code>QRCodeDetection</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a detected QR code.</p>"},{"location":"qr_reader/#vision_agent_tools.models.qr_reader.QRReader","title":"<code>QRReader</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>This tool utilizes the <code>qreader</code> library to detect QR codes within an input image. It returns a list of <code>QRCodeDetection</code> objects for each detected QR code, containing the decoded text, confidence score, polygon coordinates, bounding box, and center point.</p>"},{"location":"qr_reader/#vision_agent_tools.models.qr_reader.QRReader.__call__","title":"<code>__call__(image)</code>","text":"<p>Detects QR codes in an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image for QR code detection.</p> required <p>Returns:</p> Type Description <code>list[QRCodeDetection]</code> <p>list[QRCodeDetection]: A list of <code>QRCodeDetection</code> objects containing                    information about each detected QR code, or an empty list if none are found.</p>"},{"location":"qr_reader/#vision_agent_tools.models.qr_reader.QRReader.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the QR code reader tool.</p> <p>Loads the <code>QReader</code> instance for QR code detection.</p>"},{"location":"shared_model_manager/","title":"Shared Model Manager","text":"<p>The <code>SharedModelManager</code> class is designed to manage and facilitate the use of machine learning models across different devices, such as CPUs and GPUs, within an asynchronous environment. It ensures safe and efficient execution of these models, particularly in scenarios where GPU resources need to be shared exclusively among multiple models. The manager coordinates access to the shared GPU, preventing conflicts when multiple models require it. Models are only loaded into memory when needed using the <code>fetch_model</code> function.</p> <ul> <li><code>add()</code>: Registers a machine learning model class with the manager. The actual model instance is not loaded at this point.</li> <li><code>fetch_model()</code>: Retrieves the previously added model class and creates (loads) the actual model instance. This function utilizes PyTorch interface <code>to</code>, to handle device (CPU/GPU) allocation based on availability.</li> </ul> <p>The usage example demonstrates adding models and then using them with their respective functionalities.</p> <pre><code>model_pool = SharedModelManager()\n\n# Add models to the pool\nmodel_pool.add(QRReader)\nmodel_pool.add(Owlv2)\n\n# Read image\nimage = Image.open(\"path/to/your/image.jpg\")\n\n# Use QRReader model\nasync def use_qr_reader():\n    # Read image\n    image = Image.open(\"path/to/your/image.jpg\")\n\n    qr_reader = await model_pool.fetch_model(QRReader.__name__)\n    detections = qr_reader(image)\n    # Process detections ...\n\n# Use Owlv2 model\nasync def use_owlv2():\n    # Read image\n    image = Image.open(\"path/to/your/image.jpg\")\n\n    owlv2 = await model_pool.fetch_model(Owlv2.__name__)\n    prompts = [\"a photo of a cat\", \"a photo of a dog\"]\n    results = owlv2(image, prompts=prompts)\n    # Process results ...\n</code></pre>"},{"location":"shared_model_manager/#vision_agent_tools.tools.shared_model_manager.SharedModelManager","title":"<code>SharedModelManager</code>","text":""},{"location":"shared_model_manager/#vision_agent_tools.tools.shared_model_manager.SharedModelManager.add","title":"<code>add(model_creation_fn, device=Device.CPU)</code>","text":"<p>Adds a model to the pool with a device preference.</p> <p>Parameters:</p> Name Type Description Default <code>model_creation_fn</code> <code>callable</code> <p>A function that creates the model.</p> required <code>device</code> <code>Device</code> <p>The preferred device for the model.</p> <code>CPU</code>"},{"location":"shared_model_manager/#vision_agent_tools.tools.shared_model_manager.SharedModelManager.fetch_model","title":"<code>fetch_model(class_name)</code>  <code>async</code>","text":"<p>Retrieves a model from the pool for safe execution.</p> <p>Parameters:</p> Name Type Description Default <code>class_name</code> <code>str</code> <p>Name of the model class.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>BaseTool</code> <p>The retrieved model instance.</p>"}]}